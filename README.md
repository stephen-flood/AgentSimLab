# simple_agent

*tl;dr*
A simple, set of classes for 
* Rate limited free models (Google AI Studio)
* Basic, transparent agent and memory objects (hackable as needed)
* Basic "World" object to provide agents a sandbox where they can interact with an external environment

# 1. Model Wrapper

## Documentation
*documentation generated by o3*

A lightweight wrapper to let you focus on *ideas*, not boiler‚Äëplate. You get:
* **Text & multimodal prompts** (images, videos, URLs)
* **Function‚Äëcalling** via simple `register_tool()`
* Built‚Äëin **rate‚Äëlimit smoothing** so you don‚Äôt hit the free‚Äëtier wall

---

## 1 ¬∑ Setup (once)
```bash
pip install google-generativeai
```
```python
from google.colab import userdata  # Colab only
import os
os.environ["GEMINI_API_KEY"] = userdata.get("GEMINI_API_KEY")
```
*(In plain Python, just `export GEMINI_API_KEY="<your‚Äëkey>"` in the shell.)*

---

## 2 ¬∑ Create a model object
```python
from gemini_wrapper import GeminiModel  # or the file name you saved

freemodel = GeminiModel(
    model_name="gemini-2.0-flash-lite",  # see console for other names
    rate_limit_minutes=30,               # free‚Äëtier: 30 req/min
    rate_limit_daily=1000               # free‚Äëtier: 1‚ÄØ000 req/day
)
```
That‚Äôs it‚Äî`freemodel` is your personal assistant.

---

## 3 ¬∑ Plain‚Äëtext questions
```python
resp = freemodel.generate_content(
    user_prompt="Explain photosynthesis in 2 sentences."
)
print(resp.text)
```

---

## 4 ¬∑ Multimodal questions (images / video / URLs)
```python
resp = freemodel.multimodal_query(
    user_prompt="What‚Äôs happening in this picture?",
    attachment_names=["https://hips.hearstapps.com/hmg-prod/images/wisteria-in-bloom-royalty-free-image-1653423554.jpg"]
)
print(resp.text)
```
*Pass either local filenames* (e.g. `"my_plot.png"`) *or HTTP/HTTPS URLs.*

---

## 5 ¬∑ Calling your own Python tools
1. **Write a normal function**
```python
def get_biography(first_name: str, last_name: str) -> str:
    return f"{first_name} {last_name} was born ‚Ä¶"
```
2. **Register it**
```python
bio_tool = freemodel.register_tool(
    func=get_biography,
    description="Returns a short biography for a given person.",
    parameters={
        "first_name": {"type": "string", "description": "The person‚Äôs given name."},
        "last_name" : {"type": "string", "description": "The person‚Äôs family name."}
    }
)
```
3. **Ask the model**
```python
resp = freemodel.generate_content(
    user_prompt="Give me a one-line bio for Albert Einstein.",
    tools=[bio_tool]
)
```
4. **Execute any tool calls & print final answer**
```python
results = freemodel.apply_tool(resp)
print(results)
```
The wrapper detects `response.function_calls`, runs the right Python function, and hands back the results.

---

## 6 ¬∑ Tips & Limits
* **Rate‚Äëlimits handled** ‚Äì the wrapper pauses just enough to stay under 30 req/min.
* **Descriptions matter** ‚Äì Gemini needs full‚Äësentence parameter descriptions.
* Use **small file sizes** for images (‚â§‚ÄØ5‚ÄØMB) to keep queries snappy.
* Stick to **JSON‚Äëserialisable** argument types (strings, numbers, booleans).

---

## 7 ¬∑ Troubleshooting cheatsheet
| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| `Error: no query given` | Forgot the `user_prompt` kwarg | Provide it üòÑ |
| `Tool X not registered` | Typo in tool name | Check `register_tool` call |
| Long pause before answer | Rate limiting | It‚Äôs automatic‚Äîjust wait |

---

## 8 ¬∑ Method reference (one‚Äëliners)
| Method | What it does |
|--------|--------------|
| `generate_content(user_prompt, tools=None)` | Quick text answer (optionally with tools) |
| `multimodal_query(user_prompt, attachment_names=[])` | Answer with images / video included |
| `register_tool(func, description, parameters)` | Wrap a Python function for LLM use |
| `apply_tool(response)` | Execute any tool calls & return `(result, call)` list |
| `print_response(response)` | Convenience printer while debugging |


# Simple Agent Implementation

1. Chat function to query LLM
2. Simple Agent and Memory classes
3. Simple Framework to store environment state (e.g. World)

## Documentation

## 9 ¬∑ SimpleAgent ‚Äî one‚Äëfile conversational actor

The **`SimpleAgent`** class wraps a `GeminiModel` to produce chatty replies while keeping a tiny personal memory. No `Location` or `World` objects are required unless you add them later.

### Anatomy of `SimpleAgent`

| Attribute | Purpose | Type |
|-----------|---------|------|
| `name`    | Display name printed with each utterance | `str` |
| `traits`  | Free‚Äëtext personality/quirks | `str` |
| `status`  | Current activity / mood snippet | `str` |
| `memory`  | Rolling log of past events | `SimpleMemory` |
| `model`   | LLM interface (defaults to *gemini‚Äë2.0‚Äëflash‚Äëlite*) | `GeminiModel` |

| Method | What it does | Returns |
|--------|--------------|---------|
| `respond(observation)` | Builds a prompt, calls Gemini, parses for `SAY:` or `GOODBYE:` | `(continue_bool, [speaker, text])` |
| `description()` | Pretty one‚Äëpager of name / traits / status | `str` |
| `set_location(location)` | Placeholder for future location feature | `None` |

#### Minimal usage
```python
from gemini_wrapper import SimpleAgent, SimpleMemory

ava = SimpleAgent(
    name   = "Ava",
    traits = "helpful, concise",
    status = "idle",
    memory = SimpleMemory()
)

keep, (speaker, reply) = ava.respond(
    observation="Hi Ava! What's the capital of France?"
)
print(f"{speaker}: {reply}")
```

#### Building a chat loop
```python
def chat(agent: SimpleAgent):
    keep = True
    while keep:
        user_input = input("You: ")
        keep, (speaker, answer) = agent.respond(observation=user_input)
        print(f"{speaker}: {answer}")

chat(ava)
```

---

## 10 ¬∑ Multi‚ÄëAgent conversation

Wire together two (or more) **`SimpleAgent`** instances by feeding each agent‚Äôs line into the next agent‚Äôs `respond()` call.

```python
from gemini_wrapper import SimpleAgent, SimpleMemory

alice = SimpleAgent("Alice", "curious",     "waiting", SimpleMemory())
bob   = SimpleAgent("Bob",   "analytical",  "idle",    SimpleMemory())

def converse(a, b, max_turns=10):
    utterance = "Hello!"
    keep = True
    turn = 0
    while keep and turn < max_turns:
        speaker = a if turn % 2 == 0 else b
        keep, (who, utterance) = speaker.respond(observation=utterance)
        print(f"{who}: {utterance}")
        turn += 1
    print("\n‚ü° Conversation ended ‚ü°")

converse(alice, bob)
```

Scaling to *N* agents:
```python
agents = [alice, bob, charlie, diana]
utterance = "Hi everyone!"
keep, turn = True, 0
while keep and turn < 20:
    speaker = agents[turn % len(agents)]
    keep, (_, utterance) = speaker.respond(observation=utterance)
    print(f"{speaker.name}: {utterance}")
    turn += 1
```

---

## 11 ¬∑ Creating scenario context & background

To **train or test a specialised agent**‚Äîe.g. a customer‚Äëservice rep‚Äîseed fixed knowledge and realistic customer profiles **before** dialogue begins.

### 1Ô∏è‚É£ Product catalogue (ground‚Äëtruth knowledge)

| Key | Example | Notes |
|-----|---------|-------|
| `name` | `"ACME X100 Coffee Maker"` | Unique identifier |
| `issues` | `["leaks", "won‚Äôt power on"]` | Common problems |
| `resolutions` | `{ "leaks": "Check gasket ‚Ä¶", "won‚Äôt power on": "Verify outlet ‚Ä¶" }` | One‚Äëstep fix or RMA flow |

Load with a system prompt *or* via a `lookup_product_issue()` tool.

### 2Ô∏è‚É£ CSR & Customer agents (seeded memories)

| Agent | Memory seeds | Why? |
|-------|--------------|------|
| **CSR** | ‚Ä¢ ‚ÄúYou are a friendly ACME support rep.‚Äù<br>‚Ä¢ ‚ÄúYou can troubleshoot X100 leaks by checking the gasket ‚Ä¶‚Äù | Instant product expertise |
| **Customer** | ‚Ä¢ ‚ÄúYou bought an ACME X100 three months ago.‚Äù<br>‚Ä¢ ‚ÄúYour machine started leaking yesterday.‚Äù | Keeps customer consistent |

```python
from gemini_wrapper import SimpleAgent, SimpleMemory

csr_memory = SimpleMemory([
    "You are an ACME customer‚Äëservice representative.",
    "You can resolve X100 leaks by instructing users to check the gasket and tighten it.",
])

cust_memory = SimpleMemory([
    "You own an ACME X100 coffee maker purchased 3 months ago.",
    "Your main issue: the unit is leaking during brewing.",
    "Today is your birthday and you‚Äôre mildly upset.",
])

csr      = SimpleAgent("CSR‚ÄëBot",    "patient, empathetic", "on‚Äëshift", csr_memory)
customer = SimpleAgent("Customer‚Äë42", "frustrated",         "needs help", cust_memory)
```

### 3Ô∏è‚É£ Running the scenario

```python
utterance = "Hi, my ACME X100 is leaking all over the counter!"
turn, keep = 0, True
while keep and turn < 12:
    agent = customer if turn % 2 == 0 else csr
    keep, (who, utterance) = agent.respond(observation=utterance)
    print(f"{who}: {utterance}")
    turn += 1
```

Agents store the dialogue automatically in their private memories.

---

## 12 ¬∑ Adding Tools for outcome actions

Gemini tools let the model **trigger real Python functions**‚Äîgreat for hang‚Äëups, resolution confirmations, or refunds.

### 12.1 Backend functions

```python
def customer_hang_up():
    print("‚ö†Ô∏è  Customer disconnected in anger!")
    return "Customer has hung up."

def customer_accept():
    print("‚úÖ Issue resolved to customer‚Äôs satisfaction.")
    return "Customer is happy‚Äîcall complete."

def issue_refund(order_id: str):
    print(f"üí∏ Refund issued for order {order_id}.")
    return f"Refund processed for order {order_id}."
```

### 12.2 Register with the model

```python
hangup_tool = csr.model.register_tool(
    func=customer_hang_up,
    description="Marks the call as failed when the customer disconnects angrily.",
    parameters={}
)

accept_tool = csr.model.register_tool(
    func=customer_accept,
    description="Marks the ticket resolved when the customer is satisfied.",
    parameters={}
)

refund_tool = csr.model.register_tool(
    func=issue_refund,
    description="Processes a refund for a given order ID.",
    parameters={ "order_id": { "type": "string", "description": "E‚Äëcommerce order number." } }
)

csr_tools      = [hangup_tool, accept_tool, refund_tool]
customer_tools = [hangup_tool, accept_tool]   # customer can‚Äôt refund
```

### 12.3 Use tools during `respond()`

```python
keep, (who, utterance) = csr.respond(
    observation=prev_line,
    tools=csr_tools
)

for result, call in csr.model.apply_tool(csr.model.last_response):
    print("TOOL RESULT:", result)
```

### 12.4 Prompt guidance

Mention in the system prompt:

> ‚ÄúIf the customer indicates anger and wants to end the call, call `customer_hang_up`.  
>  If they‚Äôre satisfied, call `customer_accept`.  
>  If a refund is appropriate, call `issue_refund` with the order ID provided.‚Äù


# Simple Sandbox/World classes

## Documentation


This guide covers the **refactored `World` class** that now exposes two ready‚Äëmade
methods‚Äî`move()` and `describe_room()`‚Äîthat can be surfaced to language‚Äëmodel
agents as Gemini **tools**.

---

## 13‚ÄÇWorld at a glance
```python
class World:
    locations: list[Location]
    agents:    list[SimpleAgent]

    def move(agent_name: str, dest_name: str) -> str: ...
    def describe_room(agent_name: str) -> str: ...
```
* **`locations`** ‚Äì every `Location` object (rooms, zones, etc.)  
* **`agents`** ‚Äì every `SimpleAgent` currently in the game  

---

## 13.1‚ÄÇCore helper methods

| Method | Purpose |
| ------ | ------- |
| `clear()` | Wipe all locations & agents (fresh start). |
| `add_location(location)` | Append a `Location` you made manually. |
| `add_agent(agent)` | Append a pre‚Äëconstructed `SimpleAgent`. |
| `create_world_from_names(names, agents)` | Build all locations & agents from plain data (see ¬ß3). |
| `read_adjacency_list(pairs)` | Mark rooms as neighbours, given `[("living","kitchen"), ‚Ä¶]`. |
| `print()` | Dump every room and its inhabitants in a friendly format. |

---

## 13.2‚ÄÇFast world‚Äëbuilding

```python
location_names = ["living", "kitchen", "garden"]

agents = [
    {"name": "Susan", "traits": "tired",  "status": "working", "location": "living"},
    {"name": "Bob",   "traits": "lazy",   "status": "cooking", "location": "kitchen"},
]

world = World()
world.create_world_from_names(location_names, agents)

# Optional: wire up room connections
adjacent = [("living","kitchen"), ("living","garden"), ("kitchen","garden")]
world.read_adjacency_list(adjacent)

world.print()
```
`create_world_from_names()` **expects** each agent‚Äëdict to include  
`name`, `traits`, `status`, and `location`.

---

## 14‚ÄÇBuilt‚Äëin room actions (tool‚Äëready)

### 14.1¬†`move(agent_name, dest_name) ‚Üí str`
Checks adjacency and moves the agent if legal.

### 14.2¬†`describe_room(agent_name) ‚Üí str`
Returns a one‚Äëliner with the caller‚Äôs current room and who‚Äôs there.

---

## 15‚ÄÇExposing actions as Gemini tools
```python
def move_to_location(agent_name: str, dest: str) -> str:
    return GAME_WORLD.move(agent_name, dest)

def get_current_room_description(agent_name: str) -> str:
    return GAME_WORLD.describe_room(agent_name)

move_tool = gm.register_tool(
    func=move_to_location,
    description="Moves you to an adjacent room.",
    parameters={
        "agent_name": {"type": "string", "description": "Your own name."},
        "dest":       {"type": "string", "description": "An adjacent room."}
    }
)

describe_tool = gm.register_tool(
    func=get_current_room_description,
    description="Describes your current room and who is there.",
    parameters={
        "agent_name": {"type": "string", "description": "Your own name."}
    }
)

AGENT_TOOLS = [move_tool, describe_tool]
for agent in GAME_WORLD.agents:
    agent.tools_available = AGENT_TOOLS
```

---

## 16‚ÄÇPrompt snippet for the LLM
```
You can navigate the house.
‚Ä¢ To move, call move_to_location with your name and the destination.
‚Ä¢ To look around, call get_current_room_description.
Choose a tool call before free‚Äëtext if an action is needed.
```

*Version: April¬†2025*
